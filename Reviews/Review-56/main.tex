
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-56}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Linear Transformers Are Secretly Fast Weight Memory Systems}
\end{center}
           % ################################### %

Conventional attention mechanisms utilize the softmax operation which limits the size of memory and hinders capturing long-term temporal dependencies. Fast weight memory systems, on the other hand, provision dynamic interaction in the case of finite memories. The work leverages this insight and draws close connections between linear transformers as outer-product based fast weight memory systems. In the case of large sequence lengths, linear transformers end up in overcapacity regimes. The paper addresses this limitation by introducing a novel update rule which provisions dynamic manipulation of memory contents. The work further proposes a novel kernel function for linearised attention. Proposed update rule in conjunction with kernel empirically demonstrates improved performance to prior attention mechanisms. 

Fast weight memory systems bake context-dependencies in model parameters. A slow net with slowly changing weights generates rapidly changing weights for a \textit{fast} net. In accordance to prior works, the work highlights that outer-product based fast weight memories are akin to attention mechanism of the transformer. This can be seen by doing away with the softmax in self-attention and realizing the outer product between values $v$ and keys $k$ as fast weights. The result follows in the case of linearised self-attention. Capacity limitations of attention can be seen as the computational bottleneck which restrict retrieval of associations greater than limit of finite memory. To this end, the work improves the update rule by realizing the new stored value $v_{new}$ as a convex combination of input $v$ and retrieved value $\hat{v}$ weighed by a learnable \textit{write-strength} $\beta$. The new value $v_{new}$ acts as a dynamic write operation following the removal of prior values. Normalizing the update rule utilizing an accumulator yields \textit{attention normalization}, whereas normalizing with the sum of all query matrix elements yields the \textit{sum normalization}. 

In addition to the novel update rule, attention mechanisms incorporate the FAVOR+ and novel Deterministic Parameter-Free Projection (DPFP) kernel functions. In contrast to FAVOR+, DPFP suitably projects up the dot product dimension, hence allowing attention to scale to high-dimensional context inputs. DPFP is easy-to-compute and does away with the need for random features. Experiments further demonstrate the aptness of DPFFP in comparison to softmax and FAVOR+ on the synthetic dataset with varying sequence length. Performance of the update rule and normalization schemes is further validated on language modeling tasks wherein the novel attention mechanism improves over Performer and Linear transformers in the case of small batch sizes. However, experiments do not highlight the drawbacks of normalization scheme discussed as part of \textit{attention normalization} formulation. The normalization is theoretically insufficient to balance between write and remove operations. The paper could empirically demonstrate this claim by monitoring the contrast between retrieved and written values. Secondly, accumulation of positive values resulting in instability could be validated by observing their variation with number of steps. 

Provision of a scalable kernel function in conjunction with dynamic manipulation of values presents two new directions for future research. Firstly, the work could be extended towards alternative high-dimensional datasets and language modeling tasks which enforce variable semantic relationaships, such as question-answering. And secondly, the model could additionally incorporate novel normalizations competitive to the naive sum normalization. 

The paper explored a theoretical connection between outer-product based fast weight memories and linear attention. The relationship, through the lens of dynamic manipulation of context variables, gives rise to a novel interpolation based update rule for attention and sum normalizations. When combined with a projected kernel function, updates demonstrate improved performance with smaller batch sizes. 


\end{document}
