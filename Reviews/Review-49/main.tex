
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-49}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{NICE: Non-linear Independent Components Estimation}
\end{center}
           % ################################### %

\textit{What constitutes a good representation?} The work answers the aforesaid the question by utilizing the insight that a good representation's distribution is easy to model. Non-linear Independent Component Estimation (NICE) learns non-linear deterministic transformations of data which allow suiable distributions in latent space. Transformations carried out conform the data to factorized distributions and do away with volume preservation in the conventional layer-by-layer settings. NICE leverages a simple exact log-likelihood training criterion which is found tractable and yields apt generative representations with additional extensions to impainting.

The key idea behind NICE transformations is that the determinant of the Jacobian is trivially obtained.  Each data block can be split into separate blocks $(x_{1},x_{2}))$ which undergo transformation to form $(y_{1},y_{2})$ having arbitrarily complex functions. This block has a unit Jacobian determinant which is trivially invertible using the change of variables theorem. NICE models learn probabilit densities from a family of density distributions. This allows a tractable log-likelihood objective for generative models. The structure of models consists of 3 components. (1) Matrix inversion of the Jacobian, being a square matrix, utilizes product of upper \& lower triangular matrices $M = LU$. This allows convenient inversion of triangular matrices at test time. (2) The coupling layer composes a series of bijective transformations with trianguler Jacobians. Following the partition of input blocks, the transformation applies a coupling law which is an invertible map of the first argument given the second. Stacking of several coupling layers in series results in a more complex layered transformation. (3) Since the additive coupling layers have unit Jacobians, their compositions would remain unchanged. To do away with this volume preserving nature of transformations, the top layer of model provisions rescaling using a diagonal scaling matrix $S$. Rescaling implicitly induces non-volume preserving nature in transformations by assigning more weight on some dimensions than others. 

NICE transformations demonstrate improved likelihood estimation on challenging TFD \& CIFAR-10 datasets. By utilizing 4 coupling layers and a $\exp(s)$ rescaling, NICE represents data samples intuitively. This is validated by generated images for MNIST, TFD, SVHN \& CIFAR-10 datasets which present meaningful representations. Furthermore, transformations are found suitable for impainting of masked images using MAP inference. Although the model is not trained for this task, it suitably captures pixel regions corresponding to digits. On the other hand, transformations occasionally present spurious modes in the impainted outputs. This may be a result of the simple, yet tractable, likelihood training objective which imposes representtations to collapse on particular modes. A finer comparison between different training objectives would further throw light towards these hindrances. 

Provision of bijective non-linear transformations allows convenient computations of Jacobian, which in turn facilitate non-volume preserving representations. Given the apt density estimation process, NICE can be further extended to semi-suervised \& adversarial scenarios which depict improved sample quality in images. Furthermore, NICE models may be extended to knowledge transfer as a result of expressive representations obtained on the auxilary task of impainting. 


\end{document}
