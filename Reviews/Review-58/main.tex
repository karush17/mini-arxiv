
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-58}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{EigenGame: PCA as a Nash Equilibrium}
\end{center}
           % ################################### %

Prior methods in machine learning often misinterpret Principal Component Analysis (PCA) as a data compression technique, hence ignoring the problem of rotating within the subspace. This enforces objectives to learn the optimal subspace rather than the principal components themselves. In contrast to these approaches, the work views each principal component as a player in a game whose objective is to maximize their own utility function in controlled competition with other vectors. This leads to a novel formulation of PCA as finding the Nash Equilibrium of a suitable game. In light of the above formulation, the work proposes EigenGame, a novel sequential algorithm which approximates Nash Equilibrium on full batch data. EigenGame demonstrates globally convergence towards equilibria and distributedly scales towards higher dimensions. 

A novel perspective of PCA as a competitive game obviates the need for any orthonormalization step and is directly applicable to decentralization. The work highlights this novel view by (1) showing that only maximizing the trace of projection matrix is not sufficient for recovering all principal components and (2) minimizing the off-diagonal terms of matrix is akin to maximizing the trace and can recover all components. The derivation leverages this insight to extend the objective towards the learning of top $k$ components with $k < d$ wherein $d$ denotes the dimension of data. The aforesaid intutively results in solving an eigenvalue problem for a symmetric subspace $M$. The problem further incorporates the hierarchical dependence of top-$k$ eigenvectors and fitting tractable approximations for each of the components. This results in a set of objectives (utilities) which, for each vector $i \in \{1,2,...,k\}$, form the unique strict Nash Equilibrium. Due to the sequential nature of the problem, objectives give rise to a general algorithm whererin the utility gradient captures a generalized Gram-Schmidt step followed by the standard matrix product found in Oja's rule. Each eigenvector in the resulting EigenGame algorithm is learned by maximizing its utility by virtue of Riemanian gradient ascent within the unit sphere. Additionally, EigenGame may learn values in a decentralized fashion with each vector maximizing its utility in parallel. 

In comparison to prior methods such as GHA and Matrix Krasulinas and Oja's rule, EigenGame presents comparative convergence on synthetic and MNIST datasets for linear and exponentially decaying spectrums. Furthermore, the algorithm appropriately minimizes the distance towards its optimal subspace in the sequential setting. In case of the distributed setting, EigenGame is scaled up to retrieve the principal components of ResNet-200 activations which presents accurate reasoning about the abstract representations learned by lower layers of the architecture. Higher layers, on the other hand, learn distinct filters such as Gabor and Laplacian-of-Gaussian filter maps. 



\end{document}
