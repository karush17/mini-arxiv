
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-62}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Improved Contrastive Divergence Training of Energy-Based Model}
\end{center}
           % ################################### %

Contrastive Divergence (CD), being a poplar technique for training Energy-based Models (EBMs), is known to have difficulties with stability. The nature of the problem arises due to an extra gradient term which is often ignored in practice. The work proposes to improve CD training by adapting a framework which incorporates the missing gradient term. In the case of continuous data, the term can be efficiently computed using auto-differentiation and nearest neighbor entropy estimators. Furthermore, the work presents additional techniques such as data augmentation as a useful tool to promote mixing of Markov Chain Monte-Carlo (MCMC) chains. The proposed framework demonstrates promise for synthetic image generation, diversity in MCMC samples and stability in training. Additionally, trained EBMs are useful for downstream tasks such as image compositionality and Out-of Distribution (OOD) detection.

Utilizing CD training is a common precept in training of EBMs. However, the missing gradient term, hereby refered to as the \textit{KL Divergence term}, is often neglected due to convenience. The work proposes to utilize the KL Divergence term by interpreting its corresponding loss function via a combination of two terms. THe first term $\mathbb{E}_{q}[E_{stop\_grad}]$ and second term $\mathbb{E}_{q}[\log q]$ when combined encourage sampling to closely approximate the underlying distribution $p$. This is achieved by motivating samples to be both low energy and diverse. The first term is estimated by minimizing sampler energy using Langevin dynamics as the suitable MCMC kernel. THe second term, on the other hand, is optimized using an entropy estimator which minimizes the distance between nearest neighbors of samples. The above CD objective is coupled with EBM training wherein the energy being minimized is an aggregation of energies at different temporal resolutions. UMages are downsampled and processed using the EBM which leads to the generation of synthetic samples. The CD objective minimizes divergence between hallucinated and real samples and the difference is backpropagated into the EBM. Generation of images is carried out using Langevin sampling with data augmentation which motivates diversity and convergence of MCMC chains. Following the execution of a chain, the subsequent chain is run on an augmented sample of the last image from previous chain.

Improved CD training demonstrates numerous applications of EBMs. The framework depicts improved sample quality in comparison to prior EBM approaches and equivalent performance as GANs during unconditional generation. Langevin samples obtained after long-run MCMC chains depict stability and diverse images on the CelebA-HQ dataset. The efficacy of data augmentation is made evident when sampling from the EBM at low temperatures. In the presence of augmentation, images generated by the EBM closely resemble realistic objects. Stability of training is evaluated by monitoring KL loss which depicts convergence in the presence of self-attention and spectral norm. Furthermore, the gradient of KL term has significant magnitude and contributes to the CD term. Learned EBM further demonstrate OOD robustness by obtaining higher likelihoods in comparison to prior approaches including supervised methods. And lastly, the setup is found suitable for image compositionality wherein the learned model accurately generates class conditional images composed of a variety of attributes. 

Improved training of EBMs opens several new directions for future research. Firstly, the work motivates transfer of EBMs to other downstream and auxilary tasks such as OOD robustness in Offline Reinforcement Learning. And secondly, data augmentation in Langevin samlping motivates diverse sampling which can be further extended towards othe MCMC kernels.

\end{document}
