
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-61}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Learning Energy-Based Models by Diffusion Recovery Likelihood}
\end{center}
           % ################################### %

Learning of Energy-Based Models (EBMs) is restricted to low dimensional datasets and shorter Markov Chain Monte Carlo (MCMC) chains. Diffusion probabilistic models allow improved training and sampling as a result of the injection of white noise in the data distribution. The work leverages from recent advances in diffusion models and presents a diffusion recovery likelihood training objective for EBMs. Each EBM maximizes the conditional probability of the data at a certain noise level given their noisy versions at higher levels. Following training, images are generated by sampling from Gaussian white noise distribution and progressively samples the conditional distributions at lower noise levels. This facilitates convergence of long-run MCMC chans wherein the synthetic images continue to represent realistic images. 

The framework proposes a diffusion recovery likelihood approach for tractable learning of EBMs. Intuitively, the term \textit{recovery} indicates that the method aims to retrieve a cleaner sample $x_{i}$ from the noisy sample $\tilde{{x}}_{i}$. The sequence of marginal EBMs are learned with recovery likelihoods which are defined as the conditional distributions that invert the diffusion process. The learning process, in contrast to MLE learning, only requires sampling from the conditional distributions which is much easier than sampling from the marginal distribution. To that end, a recovery-likelihood function $\mathcal{J}(\theta)$ is trained whose distributions are much easier to sample from. Synthetic samples are generated by $K$ steps of Langevin dynamics. To facilitate sampling, learning additionally employs a normal approximation to the generated sample $x_{gen}$ which is formulated as a generalization of the reparameterization trick. The approximation, for small values of $\sigma$, provides conditional density $p(x|\tilde{x})$ which is easier to sample from and yields insights into choosing the step size for Langevin dynamics. However, sampling from $p(x|\tilde{x})$ becomes simple only when $\sigma$ is small. The framework thus learns a sequence of recovery likelihoods on gradually observed data based on the diffusion process. 

The proposed diffusion-based approach outperforms prior EBM and score-matching methods and is competitive to GAN-based frameworks on CIFAR-10 and CelebA 64 benchmarks. Qualitatively, samples generated from recovery likelihood learning are og high fidelity and close to realistic images. Latent interpolation of data points further generates images which smoothly transition between pairs, thus indicating that the diffusion process induces a smooth latent space. Additionally, the algorithm highlights its utility to the task of image impainting wherein it completes occluded regions of input images by generating apt synthetic masks. A key property of the recovery likelihood learning is highlighted in stabilizing long-run MCMC chains. The work validates this for differnt settings such as HMC sampling and variable number of samples. MCMC chains generated using the above process produce realistic images even after 1k steps. Although the sample quality of images diminishes with further sampling, the work highlights this as a promising direction for high-dimensional learning. 

Recovery likelihood learning facilitates scalability to high-dimensional datasets and opens several new avenues for future work. Firstly, the framework can be extended to other data modalities and downstream settings to study its utility for practical applications. And secondly, provision of stable long-run MCMC chains motivates investigations into its sample quality and limits on convergence.

The work introduced diffusion-based recovery likelihood learning for training of EBMs. The diffusion-based approach samples from a tractable conditional distribution and approximates generated sampples using a normal approximation. The framework is trained to maximize recovery likelihood which aids in retrieving high fidelity images with extensions to impainting and long-run MCMC chains.

\end{document}
