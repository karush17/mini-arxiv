
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-55}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Training Products of Experts by Minimizing Contrastive Divergence}
\end{center}
           % ################################### %

Modelling complex multi-modal data distributions is non-trivial with regards to multiple constraints. As opposed to the common approach of training densely connected acyclic graphs, the work presents an alternative learning structure with relaxed constraints. Product of Experts (PoE) provisions the combination of different distributions as a single produce with common renormalization. Each individual model, termed as expert, learns a certain aspect of data conditioned on a constraint. Knowledge from all experts is then combined by taking the product of their latent distributions. PoE, when trained using Contrastive Divergence (CD) minimization, demonstrates accurate approximation of data samples in comparison to conventional log likelihood training. 

Akin to domain-specific knowledge, PoE abstracts data representation across multiple experts. Each expert learns a specific aspect of data, for example a particular class or a portion of images. Experts may be trained individually or in parallel based on the learning rule. Following training of experts, a joint PoE model is constructed as a product of posterior distributions learned by each expert. In the context of Gaussian densities, a PoE may be realized as a multivariate Gaussian. The distribution is further renormalized with regards to the collective fantasy data across all experts. Training a PoE with maximum likelihood procedure is intractable as it the objective includes the gradient of log probability of fantasy data. To do away with this intractable gradient, the paper trains PoE using CD minimization which is realized as a tractable approximation to the gradient. The Markov chain implemented by Gibbs sampling is run and updated for one step which results in a distribution closer to equilibrium as compared to the initial distribution. This \textit{one-step} distribution is then utilized to approximate the gradient as an expected difference of two KL divergences both independent of fantasy data. 

PoE model training for Restricted Boltzmann Machines (RBMs) via CD minimization demonstrates suitable improvements to CD updates resulting in convergence, as opposed to conventional log likelihood training. Experts learn rich feature representations in the case of handwritten digit images such as strokes and angular curves and highlight suitable data reconstruction from binary activities. Additionally, log scores for experts are qualitatively different emphasizing the variable nature of data presented to each expert during training. On the other hand, the PoE setup could be further elaborated with regards to its approximation. While the work empirically justifies the small contribution of gradient of \textit{one-step} distribution, a theoretical reasoning for the same would validate the claim. Similarly, high-dimensional datasets may provide the model with variable manifolds which are non-trivial to optimize over. This can intuitively be observed with class outliers or large imbalanced datasets. Thus, the alternate resemblance of data samples being close to low-dimensional distributions appears weak and applicable only in the case of well-represented datasets. A more rigorous discussion in this regard would further facilitate validation. 

The PoE model presents two new avenues for future research. Firstly, the scheme can be further extended to capture richer feature representations often observed in temporal datasets such a time-series and image sequences. And secondly, the work can be further extended towards alternate training mechanisms which provision parallel feature search such as evolutionary methods. 



\end{document}
