
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-67}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Risk-Averse Offline Reinforcement Learning}
\end{center}
           % ################################### %

Various high-stakes Reinforcement Learning (RL) settings demand addressing the risk associated with exploration of the state space. With the general objective of learning safe policies from offline data, the work proposes Offline Risk-Averse Actor-Critic (O-RAAC). The O-RAAC algorithm optimizes a risk-averse criterion in a fully offline setting. This presents two benefits. On one hand, the framework produces risk-averse policies even from risk-neutral data transitions. On the other hand, the risk-averse criterion yields distributional robustness within natural distributional shifts.

The key contribution of O-RAAC framework is learning of safe policies in a fully offline setting. This is achieved by optimizing a risk-averse criterion, namely the CVaR$_{\alpha}$ using Acerbi's formulation. The algorithm consists of 3 components. (1) The distributional critic learns a distribution over values by extending the Bellman equation towards the distributional setting. This is analogous to the distributional variant of fitted value iteration using a $\tau$-quantile Huber loss. (2) The core component of O-RAAC is a risk-averse actor which minimizes the CVaR risk distortion metric using quantiles obtained from the distributional critic. The choice of CVaR is suitable as a coherent risk measure with well-defined sampled counterparts. (3) Lastly, the algorithm employs a generative model to control the bootstrapping error in the offline setting. While the preference of deterministic policies over stochastic policies is justified in regards to risk-averse optimization, stochastic policies play a key role in mitigating overfitting from fixed batch data samples. To alleviate this trade-off, the work decomposes the actor into ain imitation learning component and a perturbation model whose affine combinations yield parameterized policy. The choice of VAE as the generative framework is suitable due to its robustness towards mode collapse and prior optimality assumptions.

O-RAAC demonstrates improved risk-averse behavior in comparison to prior risk-averse and risk-neutral agents. Upon observing distributional spreads of risk events and dataset policies, O-RAAC steers the actor towards safe regions with minimum risk. Furthermore, O-RAAC agents depict average performance comparable to prior offline methods on the D4RL benchmark. As an ablation, the RAAC variant (O-RAAC without a VAE) depicts consistent validation of results on a small car exmaple. While the O-RAAC framework bakes in a risk-averse framework, its efficacy may be further strngthened with regards to task returns. For instance, the question that one may ask is \textit{"how does O-RAAC compare to state-of-the-art offline methods which carry out safe policy improvement?"} Towards this goal, the algorithm can be compared to Conservative counterparts such as CQL and COMBO. Additionally, the work lays out a distributional variant of the Bellman equation but does not provide theoretical insights towards its operation. Future directions could pave theoretical guarantees and convergence analysis of this formulation.

The O-RAAC framework, through the lens of risk-averse RL, lays two new directions for future research. Firstly, the work presents a suitable instance of risk-averse optimization within the fixed batch RL framework. This can potentially benefit application areas ranging from healthcare to safe robotics. And secondly, the framework gives rise to a key formulation of the distributional Bellman equation which can be further studdies within the context of Dynamic Programming. Distributional Bellman operators may play a potential role towards alternative formulations of optimal control.

The work presented O-RAAC, a risk-averse offline algorithm which optimizes a risk-averse criterion. O-RAAC employs a distributional critic and a risk-averse actor which is decomposed into a deterministic imitation learning component learned via VAE and a stochastic perturbation model. O-RAAC demosntrates risk-averse behaviors obtained with consistent average returns.

\end{document}
